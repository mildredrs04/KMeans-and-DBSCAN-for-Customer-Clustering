# -*- coding: utf-8 -*-
"""Hand in 4

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XGLPYwGxZVQAD71JzFCt43fth2ZIW_nY
"""

# Commented out IPython magic to ensure Python compatibility.
#Hand in 4: Clustering with KMeans and DBSCAN
#Name: Mildred RamÃ­rez
#ID: A01705162
#Date: 27/04/2023

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import random
# %matplotlib inline
from sklearn.model_selection import train_test_split 
from sklearn.datasets import make_blobs
from sklearn.preprocessing import StandardScaler
from numpy.random import uniform

from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, adjusted_rand_score
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import LabelEncoder, MinMaxScaler

from google.colab import drive

drive.mount("/content/gdrive", force_remount=True)
!pwd

# Commented out IPython magic to ensure Python compatibility.
# %cd "/content/gdrive/MyDrive/Intelligent Systems/kmeans"
!ls

"""Algorithm
1. The distance from each point to each centroid is calculated.
2. Points are assigned to their nearest centroid.
3. Centroids are shifted to be the average value of the points belonging to it. If the centroids did not move, the algorithm is finished, else repeat.
"""

#Load dataset
df = pd.read_csv('segmentation data.csv')
df

len(df)

preprocessor = Pipeline(
    [
        ("scaler", MinMaxScaler()),
        ("pca", PCA(n_components=2, random_state=42)), 
    ]
)

preprocessor.fit(df)

preprocessed_data = preprocessor.transform(df)
preprocessed_data

class KMeans:
    def __init__(self, k=3, max_iterations=100):
        self.k = k
        self.max_iterations = max_iterations

    def fit(self, data):
        self.centroids = {}
        for i in range(self.k):
            self.centroids[i] = data[i]

        for i in range(self.max_iterations):
            self.clusters = {}
            for j in range(self.k):
                self.clusters[j] = []

            for x in data:
                distances = [np.linalg.norm(x - self.centroids[c]) for c in self.centroids]
                classification = distances.index(min(distances))
                self.clusters[classification].append(x)

            previous = dict(self.centroids)
            for classification in self.clusters:
                self.centroids[classification] = np.average(self.clusters[classification], axis=0)

            isOptimal = True
            for c in self.centroids:
                original_centroid = previous[c]
                current_centroid = self.centroids[c]
                if np.sum((current_centroid - original_centroid) / original_centroid * 100.0) > 0.001:
                    isOptimal = False

            if isOptimal:
                break

    def predict(self, data):
        distances = [np.linalg.norm(data - self.centroids[c]) for c in self.centroids]
        classification = distances.index(min(distances))
        return classification

    def visualize(self, data):
        colors = ['r', 'g', 'b', 'c', 'm', 'y']  # Add more colors if needed
        fig, ax = plt.subplots()

        for classification in self.clusters:
            color = colors[classification]
            for features in self.clusters[classification]:
                ax.scatter(features[0], features[1], color=color, s=30)

        for centroid in self.centroids:
            ax.scatter(
                self.centroids[centroid][0], self.centroids[centroid][1],
                marker='o', color='k', s=100, edgecolor='k'
            )

        plt.show()

X = preprocessed_data
#X = StandardScaler().fit_transform(X)

# Create a KMeans model
model = KMeans(k=3)

# Fit the model to the data
model.fit(X)

# Visualize the clustering results
model.visualize(X)
X

class DBSCAN:
    def __init__(self, eps, min_samples):
        self.eps = eps
        self.min_samples = min_samples

    def fit(self, data):
        self.data = data
        self.labels = np.zeros(len(data))
        self.visited = np.zeros(len(data))

        cluster_label = 0
        for i in range(len(data)):
            if self.visited[i] == 0:
                self.visited[i] = 1
                cluster_points = self.region_query(i)

                if len(cluster_points) < self.min_samples:
                    self.labels[i] = -1  # Noise point
                else:
                    cluster_label += 1
                    self.expand_cluster(i, cluster_points, cluster_label)

    def region_query(self, point_idx):
        cluster_points = []
        for i in range(len(self.data)):
            if np.linalg.norm(self.data[point_idx] - self.data[i]) < self.eps:
                cluster_points.append(i)
        return cluster_points

    def expand_cluster(self, point_idx, cluster_points, cluster_label):
        self.labels[point_idx] = cluster_label

        i = 0
        while i < len(cluster_points):
            cluster_points_idx = cluster_points[i]

            if self.visited[cluster_points_idx] == 0:
                self.visited[cluster_points_idx] = 1
                new_cluster_points = self.region_query(cluster_points_idx)

                if len(new_cluster_points) >= self.min_samples:
                    cluster_points += new_cluster_points

            if self.labels[cluster_points_idx] == 0:
                self.labels[cluster_points_idx] = cluster_label

            i += 1

    def visualize(self):
        colors = ['r', 'g', 'b', 'c', 'm', 'y']  # Add more colors if needed
        fig, ax = plt.subplots()

        for i, label in enumerate(self.labels):
            if label == -1:
                color = 'k'  # Noise points
            else:
                color = colors[int(label) % len(colors)]
            ax.scatter(self.data[i, 0], self.data[i, 1], color=color, s=30)
        
        
        plt.show()

X = preprocessed_data
#X = StandardScaler().fit_transform(X)

# Create a DBSCAN model 
model = DBSCAN(eps=0.5, min_samples=2)

# Fit the model to the data
model.fit(X)

# Visualize the clustering results
model.visualize()